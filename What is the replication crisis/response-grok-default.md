The replication crisis refers to a widespread methodological and systemic issue in scientific research, particularly in fields like psychology, medicine, social sciences, and economics, where a significant proportion of published studies cannot be successfully replicated by independent researchers [1]. Replication is a cornerstone of the scientific method, involving the repetition of an experiment or study under similar conditions to verify its findings. When replications fail, it casts doubt on the reliability and validity of the original results, potentially undermining public trust in science and leading to wasted resources on flawed research [2]. This crisis gained prominence in the early 2010s but has roots in earlier concerns about research practices.

### Origins and Key Examples
The term "replication crisis" emerged prominently around 2011-2015, triggered by high-profile failures to reproduce well-known findings. For instance, in psychology, studies on concepts like "ego depletion" (the idea that willpower is a finite resource) and "power posing" (the notion that certain body postures increase confidence) were initially celebrated but later failed to replicate consistently [3]. A landmark effort was the Reproducibility Project: Psychology, which attempted to replicate 100 studies published in top journals and found that only about 36% produced statistically significant results similar to the originals, with effect sizes often much smaller [1]. Similar issues have been documented in other fields; for example, in cancer biology, a 2011-2012 initiative by pharmaceutical companies Amgen and Bayer found that only 11-25% of preclinical studies could be replicated [2]. These examples highlighted that the problem was not isolated but systemic.

### Causes of the Replication Crisis
Several interconnected factors contribute to the crisis:

1. **Publication Bias and "P-Hacking"**: Journals tend to favor novel, positive results over null or negative findings, incentivizing researchers to manipulate data (e.g., selectively reporting outcomes or adjusting analyses until results are statistically significant, known as p-hacking) [4]. This leads to an overrepresentation of false positives in the literature.

2. **Small Sample Sizes and Low Statistical Power**: Many studies use inadequate sample sizes, reducing their ability to detect true effects reliably. A study analyzing thousands of papers estimated that the median statistical power in neuroscience is only about 21%, far below the recommended 80% [5].

3. **Questionable Research Practices (QRPs)**: These include failing to report all variables measured, optional stopping (collecting data until significance is achieved), and lack of transparency in methods [3]. Surveys of scientists reveal that over 50% admit to engaging in at least one QRP [4].

4. **Incentives in Academia**: Career pressures, such as the "publish or perish" culture, reward quantity over quality, discouraging time-intensive replications [2].

5. **Field-Specific Vulnerabilities**: Softer sciences like psychology deal with complex human behaviors influenced by context, making exact replication challenging, whereas harder sciences like physics have historically higher replication rates [3].

### Impacts
The crisis has far-reaching consequences. It erodes trust in scientific findings, as seen in public skepticism toward research on topics like nutrition or climate change [2]. Economically, irreproducible research wastes billions; one estimate suggests that in the U.S. alone, over $28 billion annually is spent on preclinical research that doesn't replicate [6]. It also hampers progress, as policies and treatments based on faulty studies (e.g., in medicine) can lead to ineffective interventions [4]. However, not all views are alarmist; some argue the crisis is overstated and reflects growing pains in self-correcting science [3].

### Responses and Solutions
Efforts to address the crisis include:

- **Open Science Practices**: Initiatives like preregistration of studies (e.g., on platforms like OSF.io) to prevent p-hacking, and sharing data/code for transparency [1].

- **Replication Studies and Meta-Analyses**: Large-scale projects, such as the Many Labs series, have successfully replicated findings across labs, providing benchmarks [1].

- **Journal and Funding Reforms**: Some journals now require data sharing and prioritize replication studies. Funders like the NIH mandate rigor in grant proposals [2].

- **Education and Cultural Shifts**: Training researchers in robust statistics and rewarding replications in tenure processes [4].

While progress is being made, the crisis underscores the need for ongoing reforms to ensure science remains reliable [3].

### Sources
1. **Open Science Collaboration (2015)**: This study empirically demonstrated the extent of the replication crisis in psychology, finding low reproducibility rates and advocating for open practices to improve reliability. [Open Science Collaboration (2015)](https://doi.org/10.1126/science.aac4716)  
2. **Baker (2016)**: The author surveys scientists and highlights reproducibility issues across disciplines, viewing the crisis as a call for systemic changes like better training and incentives, while noting it's not unique to any field. [Baker (2016)](https://doi.org/10.1038/533452a)  
3. **Fanelli (2018)**: Fanelli argues that the replication crisis is real but potentially exaggerated, suggesting it's part of science's self-correcting nature and that solutions should focus on incentives rather than assuming widespread fraud. [Fanelli (2018)](https://doi.org/10.1073/pnas.1708272114)  
4. **Ioannidis (2005)**: This seminal paper posits that most published research findings are false due to biases and low power, framing the replication crisis as a predictable outcome of current practices and calling for reforms like larger studies. [Ioannidis (2005)](https://doi.org/10.1371/journal.pmed.0020124)  
5. **Button et al. (2013)**: The authors quantify low statistical power in neuroscience as a key driver of irreproducibility, urging higher standards for study design to mitigate the crisis. [Button et al. (2013)](https://doi.org/10.1038/nrn3475)  
6. **Freedman et al. (2015)**: This economic analysis estimates the high costs of irreproducible research, viewing the crisis as a resource inefficiency problem and recommending transparency measures. [Freedman et al. (2015)](https://doi.org/10.1371/journal.pbio.1002165)