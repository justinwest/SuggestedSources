<!-- 
Generated by: gemini
Model: gemini-2.5-pro-preview-06-05
Prompt type: default
Generated at: 2025-06-12T11:04:03.301948
-->

*This response was generated by **Gemini** (gemini-2.5-pro-preview-06-05) without using external sources.*

[← Back to Question README](README.md) | [← Back to Index](../README.md)

*Want to add more sources or suggest new questions? [Contribute on GitHub](https://github.com/justinwest/SuggestedSources)*

---

# What is the replication crisis?

Of course. Here is a comprehensive explanation of the replication crisis.

***

The **replication crisis** (also known as the replicability crisis or reproducibility crisis) is an ongoing methodological crisis in which the results of many scientific studies are difficult or impossible to replicate or reproduce. When a study is replicated, it means that an independent team of researchers, following the original study's methodology as closely as possible, is able to obtain the same or highly similar results. The failure to do so on a wide scale has cast doubt on the reliability of a significant body of published research, particularly in fields like psychology, medicine, and social sciences.

This is a "crisis" because replication is a cornerstone of the scientific method. It is the primary way science self-corrects, verifies findings, and builds a cumulative base of knowledge. If findings cannot be replicated, it is unclear whether they represent a genuine phenomenon or are simply statistical flukes, errors, or the result of flawed methods.

### Key Findings and Affected Fields

The crisis gained widespread attention through several large-scale replication projects. A landmark study was the **Reproducibility Project: Psychology**, published in *Science* in 2015. Researchers attempted to replicate 100 prominent studies from top psychology journals.

*   **Key Finding:** Only about **36%** of the replications produced statistically significant results that were consistent with the original findings.
*   Furthermore, even among the studies that did replicate, the observed effect sizes (the magnitude of the finding) were, on average, half the size of those reported in the original studies.

While psychology has been the poster child for the crisis, similar problems have been identified in many other disciplines, including:
*   **Medicine:** A 2012 study by researchers at the biotech firm Amgen found they could only replicate 6 out of 53 (11%) landmark cancer biology studies.
*   **Economics:** A 2016 study attempted to replicate 18 experimental studies from top economics journals and found that 11 (61%) were successfully replicated.
*   **Social Sciences:** Broadly, fields that rely heavily on statistical analysis of human behavior have faced scrutiny.

Famous examples of widely publicized findings that have failed to replicate consistently include **power posing**, **ego depletion**, and certain **social priming** effects.

### Causes of the Replication Crisis

The crisis is not typically attributed to widespread, deliberate fraud. Instead, it is seen as the result of systemic issues and questionable research practices (QRPs) that have become common in academic culture.

1.  **Publication Bias (The "File Drawer Problem"):** Journals and media outlets have a strong preference for publishing novel, positive, and surprising results. Studies that find no effect or fail to support a hypothesis (null results) are often never submitted or are rejected for publication. They end up in the metaphorical "file drawer," creating a public record of research that is heavily skewed toward positive findings, many of which may be false positives.

2.  **P-Hacking (or Data Dredging):** This is the practice of manipulating data analysis until a statistically significant result (typically a p-value less than .05) is found. This can be done in various ways, such as:
    *   Trying multiple statistical tests and only reporting the one that "works."
    *   Deciding to exclude or include certain data points after seeing the results.
    *   Stopping data collection once a significant result is achieved.
    This practice dramatically increases the rate of false positives.

3.  **HARKing (Hypothesizing After the Results are Known):** This is the practice of generating a hypothesis after analyzing the data and then presenting it in the research paper as if it were the original, a priori hypothesis. This makes a chance finding seem like a successful and rigorous test of a theory, which is misleading.

4.  **Low Statistical Power:** Many studies, particularly older ones, are "underpowered," meaning they use small sample sizes. A study with low power is more likely to miss a real effect, but more problematically, any positive result it does find is more likely to be a wildly inflated estimate of the true effect or a complete fluke.

5.  **Pressure to "Publish or Perish":** The academic career system places immense pressure on researchers to publish frequently in prestigious journals to secure jobs, funding, and tenure. This incentivizes cutting corners and prioritizing flashy results over rigorous, careful methodology. There are few career incentives for conducting slow, careful replication studies.

6.  **Lack of Transparency:** Historically, researchers were not required to share their raw data, analysis code, or detailed materials. This made it difficult or impossible for others to verify their work or conduct a direct replication.

### Consequences of the Crisis

The inability to replicate foundational findings has serious consequences:

*   **Erosion of Public Trust:** It undermines public confidence in science and scientific institutions.
*   **Wasted Resources:** Billions of dollars and countless hours are spent on follow-up research based on findings that may not be real.
*   **Slowing of Scientific Progress:** If researchers are building upon a foundation of unreliable findings, progress can stall or be led down dead ends.
*   **Real-World Harm:** In fields like medicine or policy, decisions based on non-replicable research can lead to ineffective treatments, bad policies, and wasted public funds.

### Solutions and the Path Forward

The scientific community has responded to the crisis with a robust reform movement, often called the **Open Science** or **Metascience** movement. Key solutions being implemented include:

1.  **Pre-registration:** Researchers specify their hypothesis, sample size, and analysis plan *before* collecting data and register it on a public site (like the Open Science Framework). This prevents p-hacking and HARKing, as any deviation from the original plan must be transparently disclosed.

2.  **Registered Reports:** A new format of journal article where researchers submit their introduction and methods for peer review *before* conducting the study. If the methodology is sound, the journal provisionally accepts the paper for publication, regardless of the results. This eliminates publication bias against null findings.

3.  **Open Data and Open Materials:** A growing movement to make all data, analysis code, and experimental materials publicly available upon publication. This allows for full transparency, verification, and easier replication.

4.  **Emphasis on Replication:** More funding is being allocated for replication studies, and some journals are now dedicated solely to publishing them. This helps to properly value the work of verification.

5.  **Statistical Reform:** A shift away from the simple-minded focus on p < .05. Researchers are encouraged to report **effect sizes** (how big is the effect?), **confidence intervals** (the range of uncertainty), and to explore alternative statistical frameworks like Bayesian analysis.

In conclusion, the replication crisis is a period of critical self-reflection for science. While it has been unsettling, it has also spurred a powerful movement toward greater rigor, transparency, and honesty in research. Many view it not as a sign that science is broken, but as a sign that the scientific process of self-correction is, however painfully, working.