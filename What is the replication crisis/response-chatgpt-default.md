<!-- 
Generated by: chatgpt
Model: o3
Prompt type: default
Generated at: 2025-06-08T20:21:28.247525
-->

*This response was generated by **ChatGPT** (o3) without using external sources.*

[← Back to Question README](README.md) | [← Back to Index](../README.md)

*Want to add more sources or suggest new questions? [Contribute on GitHub](https://github.com/justinwest/SuggestedSources)*

---

The replication crisis is an umbrella term for the growing realization, beginning in the early 2010s, that a substantial proportion of published scientific findings cannot be independently repeated (“replicated”) when the same or very similar methods are used.  It is not a single event but an ongoing, cross-disciplinary challenge that calls into question how research is conducted, evaluated, and communicated.

────────────────────────────────────────
1. What “replication” means  
────────────────────────────────────────
• Replication (or reproducibility) is the cornerstone of empirical science: an independent team should be able to follow an original protocol and obtain results that are statistically and substantively consistent with the original claim.  
• Direct (exact) replications try to duplicate all critical features (sample, materials, procedures).  
• Conceptual replications test the same theoretical proposition with different operational details.  
• In computational research, “reproducibility” often refers to re-running the original code and getting identical outputs, whereas “replicability” refers to collecting new data and seeing if the effect holds.

────────────────────────────────────────
2. How the crisis came to light  
────────────────────────────────────────
2000s  
  – 2005: Epidemiologist John Ioannidis publishes “Why Most Published Research Findings Are False,” arguing from first principles that low statistical power plus selective reporting virtually guarantee many false positives.  
Early 2010s (psychology flash points)  
  – High-profile failures to replicate “social priming,” ego-depletion, power-posing, and even claims of precognition.  
  – The “p-hacking” exposé by Simmons, Nelson & Simonsohn (2011).  
Large-scale, multi-lab audits  
  – Reproducibility Project: Psychology (Open Science Collaboration, 2015): only 36–39 % of 100 landmark studies replicated under a strict definition.  
  – “Many Labs” and “ManyBabies” consortia confirm that some classic effects are robust, others fragile or absent.  
Beyond psychology  
  – Genome-wide association studies (GWAS) reveal that most candidate-gene findings do not replicate (2012–2014).  
  – Amgen (2012) and Bayer (2011) report that only ~25 % of pre-clinical cancer biology papers they tried to repeat were confirmable.  
  – Economics, marketing science, political science, ecology, artificial-intelligence benchmarks, and even parts of chemistry and crystallography have since documented similar issues.

────────────────────────────────────────
3. Root causes  
────────────────────────────────────────
A. Statistical and methodological  
   • Low statistical power and small sample sizes.  
   • “p-hacking” / researcher degrees of freedom: trying multiple analyses until something is statistically significant (p < 0.05).  
   • HARKing (Hypothesizing After the Results are Known).  
   • Overreliance on null-hypothesis significance testing without effect-size reasoning or Bayesian alternatives.  
B. Publication and reporting biases  
   • Journals favor novel, positive findings; null results languish in the “file drawer.”  
   • Selective outcome reporting (only a subset of measured variables are published).  
C. Incentive structures  
   • “Publish or perish” rewards quantity, novelty, and journal prestige rather than rigor and transparency.  
D. Practical issues  
   • Poor documentation of methods, materials, code, and data.  
   • Context sensitivity of many social and biological phenomena.  
   • Proprietary data sets or reagents that are hard to obtain.

────────────────────────────────────────
4. Consequences  
────────────────────────────────────────
• Credibility: Erodes public and intra-scientific trust.  
• Opportunity cost: Resources are wasted chasing false leads (e.g., drug development).  
• Policy missteps: Education, health, and social programs have been based on unreplicable evidence.  
• Meta-science growth: Entire sub-field (“science of science”) now studies how research works.

────────────────────────────────────────
5. Proposed and ongoing reforms  
────────────────────────────────────────
1. Transparency  
   – Mandatory data, code, and materials sharing (FAIR principles).  
   – Open-science badges; journal policies (e.g., TOP Guidelines).  
2. Preregistration & Registered Reports  
   – Time-stamped study and analysis plans; journals accept papers before results are known.  
3. Incentive realignment  
   – Funding and hiring criteria that reward rigor, openness, and validated replication.  
4. Statistical improvements  
   – Formal power analyses, correction for multiple comparisons, Bayesian estimation, multiverse analyses.  
5. Dedicated replication funding & journals  
   – e.g., Center for Open Science grants; journals such as PLOS ONE, Royal Society Open Science, and special replication sections in major outlets.  
6. Large collaborative replications  
   – “Many Labs,” Psychological Science Accelerator, RepliCATS, Cancer Biology Replication Project.  
7. Education & culture change  
   – Curriculum emphasizes reproducible workflows (version control, literate programming, registered code).  

────────────────────────────────────────
6. Current status (2020s)  
────────────────────────────────────────
• The crisis is now widely acknowledged; transparency and replication norms have improved, especially in psychology, economics, and biomedical science.  
• Early signs show higher replication rates when preregistration, larger samples, and stricter methods are used.  
• Some subfields (e.g., large-scale genetics, high-energy physics) already exhibit strong reproducibility, showing that the problem is solvable when incentives and methods align.  
• The debate has matured from “Is there a crisis?” to “Which practices maximize cumulative, self-correcting science?”

────────────────────────────────────────
7. Take-home summary  
────────────────────────────────────────
The replication crisis reflects systemic flaws—not individual bad actors—in how modern science incentives novelty, handles statistics, and communicates results.  Recognizing the crisis has prompted a broad reform movement whose core principles are transparency, rigorous design, adequate statistical power, and realignment of academic incentives.  While not yet solved, these changes are already improving the reliability and credibility of scientific knowledge.