**Overview:** The *replication crisis* (or reproducibility crisis) refers to a pervasive problem where many scientific studies cannot be replicated by independent researchers, calling into question the reliability of those findings. In science, a result isn’t truly solid until it can be repeated under the same conditions, so widespread failures to replicate undermine confidence in published research ([www.ebsco.com](https://www.ebsco.com/research-starters/science/replication-crisis#:~:text=The%20replication%20crisis%2C%20also%20known,integrity%20of%20scientific%20inquiry%20itself)) ([iiab.me](https://iiab.me/kiwix/content/wikipedia_en_all_maxi_2023-10/A/Replication_crisis#:~:text=Replication%20crisis)). The crisis gained prominent attention in the early 2010s after several high-profile findings – especially in psychology and social sciences – fell apart when retested. For example, a famous 2011 study claimed evidence for “precognition” (people ostensibly sensing future events), but other researchers **could not reproduce its results**, casting doubt on such extraordinary claims ([www.ebsco.com](https://www.ebsco.com/research-starters/science/replication-crisis#:~:text=independently%20verified,integrity%20of%20scientific%20inquiry%20itself)). In 2015, a large collaborative project tried to replicate 100 psychology experiments published in top journals: **over two-thirds of those replications failed to get the same result as the original studies** ([www.ebsco.com](https://www.ebsco.com/research-starters/science/replication-crisis#:~:text=In%202015%2C%20a%20pivotal%20article,methodologies%20used%20across%20various%20disciplines)). Similar patterns have been observed elsewhere. In fields like biomedicine, *independent checks of important findings have often failed*: one analysis of landmark cancer biology papers found only ~11% could be confirmed by follow-up experiments ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=Data%20on%20how%20much%20of,generally%20showing%20the%20most%20confidence)). In a 2016 survey of 1,576 scientists (across chemistry, biology, medicine, physics, psychology, etc.), **over 70% admitted they had tried and failed to reproduce another scientist’s results**, and ~50% even failed to reproduce their *own* results. More than half of those surveyed agreed that science faces a significant “reproducibility crisis” ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=Survey%20sheds%20light%20on%20the,%E2%80%98crisis%E2%80%99%20rocking%20research)) ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=Although%2052,still%20trust%20the%20published%20literature)). These revelations sparked widespread concern because they suggest that a portion of published findings – even in leading journals – might not be as reliable as assumed.

**Causes of the Crisis:** Researchers have scrutinized why so many studies don’t replicate, and they’ve uncovered a **combination of systemic issues** in how science is conducted and published. Key factors include:

- **Lack of replication culture:** Traditionally, very few studies in many fields were ever independently replicated or published as replication attempts. Novel findings get more attention and reward than confirmations. This *“publication bias”* means journals have favored positive, exciting results and have often **ignored studies that rerun experiments or report null findings**, leaving errors uncorrected ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=1,Bakker%20%26%20Wicherts%202011)) ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=The%20associated%20open%20science%20reform,significant%20results)).  
- **Questionable research practices:** A high prevalence of flexible and selective data analysis techniques has inflated false positives in the literature ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=3,2016)). Researchers under pressure might engage in *p-hacking* – **analyzing data in multiple ways until something becomes statistically significant**, then reporting only that favorable result – or *HARKing* (formulating a hypothesis *after* knowing the results). They may cherry-pick data or tweak experimental conditions without disclosing it. These practices can create seemingly significant findings that are actually just flukes. Over time, such unchecked practices contributed to many non-reproducible “discoveries” ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=3,2016)).  
- **Statistical power and sample size issues:** Many studies, especially in social and biomedical sciences, have used **small sample sizes or weak experimental designs** that lack the statistical power to detect true effects reliably ([journals.plos.org](https://journals.plos.org/plosmedicine/article?id=10.1371%2Fjournal.pmed.0020124#:~:text=There%20is%20increasing%20concern%20that,involved%20in%20a%20scientific%20field)) ([journals.plos.org](https://journals.plos.org/plosmedicine/article?id=10.1371%2Fjournal.pmed.0020124#:~:text=in%20chase%20of%20statistical%20significance,conduct%20and%20interpretation%20of%20research)). A consequence is that published effects are often exaggerated or spurious. In 2005, epidemiologist John Ioannidis famously showed via modeling that, under typical research conditions (small studies, flexible analyses, many teams racing for novel results), it’s easy to land on false positives – to the point that he provocatively concluded **“most published research findings are false”** if no corrective measures are taken ([journals.plos.org](https://journals.plos.org/plosmedicine/article?id=10.1371%2Fjournal.pmed.0020124#:~:text=Published%20research%20findings%20are%20sometimes,problem%20and%20some%20corollaries%20thereof)) ([journals.plos.org](https://journals.plos.org/plosmedicine/article?id=10.1371%2Fjournal.pmed.0020124#:~:text=,claimed%20research%20findings%20are%20false)). This theoretical warning aligns with the empirical replication failures observed later.  
- **Perverse incentives in science:** The professional culture in academia has long prioritized **quantity and novelty of publications over quality**. Career pressure to “publish or perish” encourages scientists to pursue surprising, newsworthy findings and to **avoid spending time on confirmation or replication studies** that journals might reject as unoriginal ([www.news-medical.net](https://www.news-medical.net/life-sciences/What-is-the-Replication-Crisis.aspx#:~:text=reproduced%20it%20cannot%20be%20validated,impacts%20every%20natural%20science%20field)) ([www.news-medical.net](https://www.news-medical.net/life-sciences/What-is-the-Replication-Crisis.aspx#:~:text=There%20are%20a%20few%20causes,desire%20to%20publish%20or%20perish)). This skewed incentive system has historically meant fewer checks on initial results and more reward for potentially shaky positive findings.  
- **Poor transparency:** Important details needed to replicate experiments – such as full methodologies, raw data, and analysis code – have often been **missing or withheld** in publications ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=rate%20of%20false%20positives%20in,2016)). Without access to the original data or a clear, complete methods description, other scientists struggle to redo the study in exactly the same way. This lack of transparency makes replication failures more likely and harder to diagnose (one can’t tell if a replication deviated in some critical way or if the original was flawed).  
- **Rare but real misconduct:** In a small minority of cases, irreproducible results are due to outright *fraud* – data fabrication or falsification. Surveys suggest that only around 2% of scientists admit to having faked data, which implies deliberate fraud is **not the primary driver** of the replication crisis ([www.pnas.org](https://www.pnas.org/doi/full/10.1073/pnas.1708272114#:~:text=Scientific%20misconduct%20and%20questionable%20research,to%20a%20survey%20among%20psychologists)) ([www.pnas.org](https://www.pnas.org/doi/full/10.1073/pnas.1708272114#:~:text=appear%20to%20corroborate%20this%20conclusion,6%2C%207)). (By contrast, the questionable practices and biases above are much more common.) Still, when fraud occurs it can lead to spectacular failures to replicate and high-profile retractions, further eroding trust.

All these factors together create a perfect storm: many published findings were never rigorously checked or were pushed through biased analyses, so it’s not surprising that when scientists later try to replicate those experiments with more careful methods, **a large proportion fail to confirm the original claims** ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=1,completeness%20in%20the%20reporting%20of)) ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=3,2016)). The crisis, therefore, isn’t usually that scientists are deliberately cheating; it’s that the traditional way of doing science left too much room for bias, error, and one-off results to slip into the literature as “facts.”

**Efforts to Improve Reproducibility:** The upside of the replication crisis is that it has triggered a reform movement in science. In response to the alarm, researchers, institutions, and journals are implementing changes to **increase transparency and reliability**. Key initiatives include:

- **Pre-registration of studies:** Scientists are now encouraged to *pre-register* their experimental plans and hypotheses in advance (e.g. in public registries) ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=The%20associated%20open%20science%20reform,significant%20results)). Declaring methods and analysis plans beforehand makes it harder to later cherry-pick favorable results, thus tackling p-hacking. Journals like *Psychological Science* and others have started requiring or rewarding pre-registered research, which should yield more credible, less biased results.  
- **Open data and materials:** There’s a new emphasis on **sharing raw data, code, and detailed protocols** openly so that other researchers can reproduce the analysis and verify findings ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=The%20associated%20open%20science%20reform,significant%20results)). Many journals and funders now have policies mandating data availability. By allowing anyone to re-run the analysis, errors can be spotted and corrected, and true results can be confirmed more easily ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=rate%20of%20false%20positives%20in,2016)) ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=The%20associated%20open%20science%20reform,significant%20results)).  
- **Replication and publication of null results:** The scientific community is working to **normalize replication studies and the reporting of negative findings**. New journal formats like *Registered Reports* guarantee publication of a study based on the soundness of its protocol rather than the excitement of its outcome – meaning that even if results turn out “null” or not novel, they’ll be published ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=The%20associated%20open%20science%20reform,significant%20results)). Top journals have begun *publishing high-quality replication attempts* (successful or not) and even special issues on reproducibility. Over time, this helps correct the literature and signals that **confirmatory work is valued**, not just flashy discoveries.  
- **Higher statistical standards:** Some researchers advocate using more stringent criteria for statistical significance (for instance, **lowering the p-value threshold for claims** from 0.05 to 0.005 in certain fields) to reduce false positives ([www.news-medical.net](https://www.news-medical.net/life-sciences/What-is-the-Replication-Crisis.aspx#:~:text=There%20are%20a%20few%20causes,desire%20to%20publish%20or%20perish)). There’s also better awareness of using larger sample sizes and more rigorous experimental designs to ensure adequate statistical power ([www.ebsco.com](https://www.ebsco.com/research-starters/science/replication-crisis#:~:text=replications%20of%20published%20psychology%20studies,methodologies%20used%20across%20various%20disciplines)). Together these practices mean new findings will be more robust and less likely to be flukes.  
- **Education and cultural change:** The crisis has prompted improvements in how scientists are trained in statistics and methodology. There’s growing exposure of young researchers to the ideas of open science, ethics, and replication. *Transparency, replication, and methodological rigor are increasingly being incentivized* – for example, some funding agencies now specifically allocate grants for replication studies, and hiring/promotion committees may give credit for open science practices. The overall aim is to realign the incentives of science with truth-seeking: rewarding careful, reproducible research as much as novel breakthroughs ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=The%20associated%20open%20science%20reform,significant%20results)) ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=meta,there%20on%20the%20values%20and)).

Notably, the scrutiny of this “crisis” has given birth to the field of *metascience* (or meta-research) – the scientific study of scientific practice itself ([britac.co.uk](https://britac.co.uk/replication-crisis-wikipedia/#:~:text=not%20to%20be%2C%20the%20reasons,repeating%20the%20experiment%20or%20study)). Metascience researchers empirically examine issues like reproducibility, publication bias, and methodology across disciplines, providing evidence to guide reforms. In a sense, science is turning the lens on itself to understand where things go wrong and how to fix them.

**Perspectives and Ongoing Debate:** There is broad consensus that improving reproducibility is crucial, but experts differ on how severe the problem truly is and how to characterize this period of change. **One view** is that the replication crisis exposed serious, widespread flaws in research that *must* be addressed to restore credibility. This camp often uses the language of crisis and has been instrumental in pushing for reforms. For example, Ioannidis’s 2005 paper bluntly argued that most published findings might be false, highlighting how pervasive the problems could be ([journals.plos.org](https://journals.plos.org/plosmedicine/article?id=10.1371%2Fjournal.pmed.0020124#:~:text=Published%20research%20findings%20are%20sometimes,problem%20and%20some%20corollaries%20thereof)) ([journals.plos.org](https://journals.plos.org/plosmedicine/article?id=10.1371%2Fjournal.pmed.0020124#:~:text=,claimed%20research%20findings%20are%20false)). Many psychologists and biologists have since acknowledged a kind of reckoning: longstanding practices were producing lots of unreliability, so substantial change is needed to rebuild trust in the literature.

**Another view** is more nuanced, suggesting that claims of “science in crisis” are overstated or too blanket. Some scholars argue that while reproducibility issues exist, **they may be confined to certain fields or a subset of studies rather than infecting all of science** ([www.pnas.org](https://www.pnas.org/doi/full/10.1073/pnas.1708272114#:~:text=contemporary%20scientific%20profession)) ([www.pnas.org](https://www.pnas.org/doi/full/10.1073/pnas.1708272114#:~:text=I%20argue%20that%20this%20crisis,as%20for%20the%20reproducibility%20agenda)). For instance, a 2018 analysis by Daniele Fanelli questioned the doomsday narrative: it pointed out that a **majority of published results in many areas might still be valid**, that the prevalence of faulty research varies between subfields, and that there isn’t clear evidence that things have been getting worse over time ([www.pnas.org](https://www.pnas.org/doi/full/10.1073/pnas.1708272114#:~:text=I%20argue%20that%20this%20crisis,as%20for%20the%20reproducibility%20agenda)) ([www.pnas.org](https://www.pnas.org/doi/full/10.1073/pnas.1708272114#:~:text=Is%20there%20a%20reproducibility%20crisis,and%20listed%20in%20Dataset%20S1)). Fanelli and others also note that science has a self-correcting nature – the very fact that we’re discovering irreproducible results and discussing them is a sign that science is working as it should, weeding out errors ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=The%20results%20capture%20a%20confusing,%E2%80%9D)) ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=The%20P%20value%20,it%27s%20cracked%20up%20to%20be)). From this perspective, using the term “crisis” for *all* of science could be counterproductive or too pessimistic. They prefer to emphasize the positive steps being taken and remind that **many findings do replicate** (for example, the Nature survey found most scientists still trust the bulk of literature in their field, despite the concerns ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=Although%2052,still%20trust%20the%20published%20literature)) ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=Data%20on%20how%20much%20of,generally%20showing%20the%20most%20confidence))).

Indeed, **a third perspective** reframes the situation not as an existential crisis, but as a *“credibility revolution”* in science. Rather than viewing reproducibility problems as a disaster, many see the current era as an opportunity to greatly strengthen scientific practice. This outlook, championed by leaders of the open-science movement, stresses the **improvements in openness, rigor, and collaboration** that have emerged in response to past failures ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC11290608/#:~:text=The%20emergence%20of%20large,impact%20on%20our%20research%20environment)) ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC11290608/#:~:text=sciences%20to%20experience%20a%20so,impact%20on%20our%20research%20environment)). For example, psychologist Simine Vazire suggested that the reforms sparked in psychology constitute a credibility revolution – a fundamental shift toward transparency and reliability that will ultimately *increase* confidence in research findings ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=Collaboration%2C%20OSC%202015%29,science%20practices%20it%20has%20motivated)) ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=facing%20a%20%E2%80%9Creplication%20crisis%E2%80%9D%20,science%20practices%20it%20has%20motivated)). In 2023, a group of researchers even argued that the so-called replication “crisis” has led to **positive structural changes** in how science is done – from better training and statistical methods to more inclusive, multi-center studies – which will have long-term benefits for the research community ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC11290608/#:~:text=The%20emergence%20of%20large,impact%20on%20our%20research%20environment)) ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC11290608/#:~:text=reframe%20this%20%E2%80%98crisis%E2%80%99%20through%20the,changes%20which%20will%20have%20a)). According to this view, the heavy scrutiny on replication is a sign of a healthy scientific process correcting itself, and while it may be challenging, it’s not a cause to despair about science – it’s a reason to *improve* science.

In summary, the replication crisis has been a wake-up call that **shook several fields into evaluating their practices**. It revealed that many published results were less solid than we thought. In response, scientists are changing how they work – improving methods, encouraging transparency, and double-checking important findings more often. There’s active debate about the extent of the problem, but nearly everyone agrees on its core message: **replicability is crucial for science**, and by confronting this challenge head-on, researchers can ensure that scientific knowledge is built on more reliable foundations for the future ([www.news-medical.net](https://www.news-medical.net/life-sciences/What-is-the-Replication-Crisis.aspx#:~:text=The%20replication%20crisis%2C%20also%20known,or%20theory%20undermine%20its%20credibility)) ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=The%20associated%20open%20science%20reform,significant%20results)).

**Sources:**

1. **Cait Caffrey (2024)** – *“Replication crisis”* – **EBSCO Research Starters** (Science). This source defines the replication crisis as the difficulty of reproducing many scientific results and notes it became prominent in the early 2010s after multiple high-profile studies (e.g. a claim of precognition) failed to replicate. It explains that replication is a cornerstone of scientific validity and describes a 2015 effort which found over two-thirds of tested psychology findings did not hold up. Caffrey discusses suspected causes (questionable research practices, small sample sizes, lack of data access) and proposed solutions like pre-registering studies, larger samples, and open data. *URL:* **_[EBSCO](https://www.ebsco.com/research-starters/science/replication-crisis)_**

2. **Monya Baker (2016)** – *“1,500 scientists lift the lid on reproducibility”* – **Nature News**. Baker reports on a large survey about the reproducibility “crisis” in science. Notably, **70%** of surveyed researchers said they had failed to reproduce another scientist’s experiments (and ~50% failed to reproduce their own work). **52%** of respondents characterized the situation as a significant crisis. The article also highlights concrete evidence of the problem: previous analyses found that only around **40%** of psychology studies and **~10%** of cancer-biology studies could be replicated. Baker explores scientists’ mixed attitudes — many acknowledge the problem while still trusting literature overall — and frames the reproducibility issue as a broad, cross-disciplinary challenge now coming to light. *URL:* **_[Nature](https://www.nature.com/articles/533452a)_**

3. **John P. A. Ioannidis (2005)** – *“Why Most Published Research Findings Are False.”* – **PLoS Medicine 2(8): e124**. In this influential essay, Ioannidis uses statistical arguments to claim that a majority of published research results might be **false positives**. He identifies factors that undermine reliability: small study sizes, tiny effect sizes, flexibility in study designs and analyses, researcher bias, and the sheer number of hypotheses tested. His simulations and theoretical model suggest that under typical conditions (with prevalent biases and errors), the *probability that a research claim is true can be quite low*. This provocative paper sounded an early alarm about systemic issues in scientific research; it essentially anticipated the replication crisis by arguing that many findings won’t replicate because they were never true results to begin with. *URL:* **_[PLoS Medicine](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)_**

4. **Daniele Fanelli (2018)** – *“Is science really facing a reproducibility crisis, and do we need it to?”* – **Proceedings of the National Academy of Sciences (PNAS) 115(11): 2628–2631**. Fanelli’s article challenges the narrative of a universal replication crisis. He acknowledges that reproducibility and integrity issues exist, but presents evidence that **these problems likely affect a minority of published studies and are not worsening over time**. The paper argues that the “science is broken” crisis rhetoric is exaggerated; for instance, surveys indicate outright data fraud is rare (~1–2% of scientists admit to falsification), and not all fields are equally affected by replication difficulties. Fanelli suggests that a more accurate and inspiring narrative is one of *improvement and empowerment*: science is undergoing reforms to address issues, rather than spiraling into dysfunction. His view provides a counter-balance, emphasizing that most science remains sound and that recent initiatives are strengthening research reliability. *URL:* **_[PNAS](https://www.pnas.org/doi/full/10.1073/pnas.1708272114)_**

5. **Stanford Encyclopedia of Philosophy (2018)** – *“Reproducibility of Scientific Results”* (entry by **Leonelli & Fidler** et al.). This comprehensive overview breaks down what the replication (reproducibility) crisis entails and its implications. It outlines multiple facets of the problem: the historical lack of published replication studies, evidence of widespread **publication bias**, the high prevalence of **questionable research practices** (like p-hacking and selective reporting) that inflate false findings, and deficiencies in transparency (incomplete reporting of methods and data). The entry also describes the rise of the **open science reform movement** designed to combat these issues – for example, encouraging data sharing, preregistration of studies, and journals accepting null results and replications. Notably, the authors mention that some researchers prefer the term “**credibility revolution**” to highlight the positive changes (improved methods, more openness) sparked by the crisis, rather than focusing on pessimistic aspects. This source provides a balanced, scholarly summary of the crisis and the ongoing reforms. *URL:* **_[Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/scientific-reproducibility/)_**

6. **Max Korbmacher *et al.* (2023)** – *“The replication crisis has led to positive structural, procedural, and community changes.”* – **Communications Psychology 1, Article 3**. Korbmacher and colleagues present a *Perspective* piece that reframes the replication crisis in a constructive light. They argue that what has been called a crisis is actually driving a “**credibility revolution**” in science. The paper outlines how large-scale replication projects (which initially revealed low reproducibility rates) became a catalyst for **significant improvements** in research culture and practice. These authors highlight reforms such as better training in statistics and methodology, new publication practices (like Registered Reports), collaborative multi-lab studies, and greater diversity and inclusion in research efforts. The overall view expressed is optimistic: the challenges exposed by the replication crisis are being met with concerted action, leading to long-term structural changes that will markedly improve the trustworthiness of scientific research. *URL:* **_[Communications Psychology via PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11290608/)_**