<!-- 
Generated by: chatgpt
Model: o3
Prompt type: sources
Generated at: 2025-06-08T20:21:23.145961
-->

*This response was generated by **ChatGPT** (o3) using the provided sources.*

[← Back to Question README](README.md) | [← Back to Index](../README.md)

*Want to add more sources or suggest new questions? [Contribute on GitHub](https://github.com/justinwest/SuggestedSources)*

---

Definition  
The “replication crisis” (sometimes called the reproducibility crisis) refers to the growing realization, beginning around 2011, that a large share of published scientific findings cannot be reproduced when independent teams repeat the experiments or re-analyze the data.  The problem was first documented in psychology and the social sciences, but similar patterns have since been reported in biomedicine, economics, and other fields.  A result that cannot be reproduced casts doubt on its validity and, by extension, on the reliability of the scientific literature in which it appears.

Key empirical evidence  
1.  Psychology.  The Reproducibility Project: Psychology re-ran 100 prominent studies and found that only 36 % produced a statistically significant result in the same direction as the original; effect sizes were, on average, half as large [1].  
2.  Broader surveys.  Meta-analyses and multi-lab projects now suggest that roughly 60-80 % of new psychological claims will fail a direct replication, prompting commentators to write that “about 75 % of psychology claims are false” [2].  
3.  Biomedicine.  High-impact but fraudulent or irreproducible Alzheimer’s papers misdirected a decade of drug development, illustrating that the crisis extends beyond psychology [3].  

Why do so many studies fail to replicate?  
•  Questionable research practices: p-hacking, selective reporting, HARKing (“Hypothesizing After Results are Known”) inflate false-positive rates [4].  
•  Publication bias: Journals prefer novel, positive findings, giving little space to null results or replications [1][6].  
•  Small, under-powered samples: Low statistical power means genuine effects are less likely to be detected and exaggerated when they are [6].  
•  Perverse incentives: Careers and funding depend on eye-catching results, not on careful confirmation [5].  
•  Occasional outright fraud: Although rarer than sloppiness, high-profile fabrications magnify wasted effort and erode trust [3][5].

Consequences  
•  Wasted resources: Time and billions of dollars are spent following up on false leads (e.g., Alzheimer’s β-amyloid fraud) [3][5].  
•  Policy and clinical errors: Irreproducible basic science can misinform public policy or medical guidelines.  
•  Erosion of public trust: Each high-profile failure feeds skepticism about science more broadly.  
•  Calls for reform: Journals, funders, and universities are revising incentives toward transparency and replication.

Proposed (and partially implemented) solutions  
1.  Preregistration of hypotheses and analysis plans [4][6].  
2.  Open data, open materials, and registered reports (journal commits to publish regardless of outcome).  
3.  Larger, multi-site replication consortia that share protocols and pool data [1].  
4.  Statistical education emphasizing power analysis and robust methods [6].  
5.  Stronger consequences for fraud, including possible criminal penalties for egregious cases [5].

Bottom line  
The replication crisis is not a single scandal but a systemic problem: the scientific ecosystem rewards surprising, positive results more than careful verification.  Empirical surveys now show that a sizable fraction of influential findings—especially in psychology and biomedicine—do not hold up under scrutiny, prompting a wave of methodological reforms aimed at restoring reproducibility, credibility, and public trust in science.

Sources  
[1] Open Science Collaboration. “Estimating the Reproducibility of Psychological Science.” Science 2015. Shows only 36 % of 100 high-profile psych studies replicated; cites low power, publication bias. https://pubmed.ncbi.nlm.nih.gov/26315443/  
[2] “~75 % of Psychology Claims Are False.” Unsafe Science (Substack) 2023. Opinion piece arguing, from meta-replication data, that roughly three-quarters of new results fail. https://unsafescience.substack.com/p/75-of-psychology-claims-are-false  
[3] Carroll, A. “The Long Shadow of Fraud in Alzheimer’s Research.” New York Times Op-Ed, 24 Jan 2025. Argues that irreproducible (and possibly fraudulent) β-amyloid images stalled Alzheimer’s drug progress, illustrating replication crisis in biomedicine. https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html  
[4] Inzlicht, M. “Revisiting Stereotype Threat: A Reckoning for Social Psychology.” 2024 pre-print. Reviews failure of stereotype-threat effects to replicate; advocates preregistration and open data. https://www.factfaq.com/resources/Revisiting_Stereotype_Threat_-_by_Michael_Inzlicht.pdf  
[5] Matthews, D. “The Staggering Death Toll of Scientific Lies.” Vox, 2024. Explores how fraudulent and irreproducible biomedical findings can cost lives; suggests tougher penalties. https://www.vox.com/future-perfect/368350/scientific-research-fraud-crime-jail-time  
[6] Ioannou, A. & Papadopoulos, N. “From Reproducibility Crisis to Reform: Practical Steps for Robust Science.” Nature News Feature 2023 (doi:10.1038/d41586-023-02299-w). Summarizes causes (small samples, p-hacking) and solutions (registered reports, data sharing). https://www.nature.com/articles/d41586-023-02299-w  
Additional background review of fixes and statistical recommendations: Johnson et al., “Improving Reproducibility in the Life Sciences” (open-access review). https://pmc.ncbi.nlm.nih.gov/articles/PMC10581498/