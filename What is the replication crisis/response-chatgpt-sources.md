The **replication crisis** (also called the reproducibility crisis) refers to a significant problem in science: a large number of studies have proven difficult or impossible to replicate, calling into question the reliability of their results. In 2005, epidemiologist John Ioannidis famously argued that most published research findings could actually be false, due to factors like small study sizes, biases, and flexible analyses ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC1182327/#:~:text=There%20is%20increasing%20concern%20that,is%20greater%20financial%20and%20other)). A few years later, a series of failures to replicate high-profile findings (especially in psychology) led scientists to warn of a “replication crisis” in research. The term reflects growing concerns that many published results cannot be reproduced by independent researchers, undermining confidence in those findings. By 2016, even the broader scientific community acknowledged the problem: a **Nature** survey of 1,576 researchers found that 52% agreed there is a “significant” crisis of reproducibility in science ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=The%20data%20reveal%20sometimes,that%20they%20still%20trust%20the)). 

**Empirical evidence of the crisis:** In a landmark 2015 project, the Open Science Collaboration attempted to replicate 100 psychology experiments from top-tier journals; the replicators were able to reproduce the original results in less than half of the cases ([www.science.org](https://www.science.org/doi/10.1126/science.aac4716#:~:text=decline.%20Ninety,than%20by%20characteristics%20of%20the)). Specifically, whereas 97% of the original studies had statistically significant findings, only 36% of the replication attempts achieved significant results – and the replicated effect sizes were on average roughly half the magnitude of the originals ([www.science.org](https://www.science.org/doi/10.1126/science.aac4716#:~:text=0,that%20replication%20success%20was%20better)). Similarly sobering results have appeared in other fields. For example, pharmaceutical researchers at Amgen tried to reproduce 53 “landmark” cancer biology studies but succeeded in only 6 cases (~11% reproducibility) ([brokenscience.org](https://brokenscience.org/raise-standards-for-preclinical-cancer-research/#:~:text=research%2C%20focusing%20on%20the%20lack,The)).  Replicability problems have also been documented in economics, neuroscience, and other disciplines. Consistent with these patterns, many scientists report personal experience with failed replications: in the same 2016 survey, more than 70% of researchers said they had tried and failed to reproduce another scientist’s experiments (and over half had failed to reproduce their own results) ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=More%20than%2070,questionnaire%20on%20reproducibility%20in%20research)). These findings suggest that a substantial portion of published results may be **unreliable or exaggerated**. 

**Causes of the replication crisis:** Researchers have identified several contributing factors. One major issue is **publication bias** – the tendency for journals to publish positive, novel findings while leaving negative or inconclusive results unpublished. This bias skews the literature toward exciting outcomes, making it appear that effects are more robust than they really are. In addition, many scientists have engaged in **questionable research practices** that inflate false positives. Examples include *p-hacking* (analyzing data in many ways until a significant result appears), *HARKing* (forming hypotheses after results are known), and selective reporting of only favorable experiments. Such practices can produce striking findings that are actually spurious. As Ioannidis noted, studies are less likely to report true results when they use small samples, have flexible designs or analyses, or are conducted amid strong biases and competition – conditions that were quite common in many fields ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC1182327/#:~:text=There%20is%20increasing%20concern%20that,is%20greater%20financial%20and%20other)). Low statistical power (insufficient data) in many studies meant that even true effects could be missed, while false positives could slip through. All of these factors – combined with pressure on researchers to publish frequently and produce novel findings – created fertile ground for non-replicable results in the literature. 

**Responses and perspectives:** The recognition of the replication crisis has prompted a range of reforms and some debate about its implications. On one hand, the scientific community has embraced **open science** practices to improve reproducibility. Many journals and institutions now encourage steps like preregistering study designs (to prevent HARKing), sharing data and code, conducting replication studies, and using more rigorous statistical criteria. These initiatives aim to make research more transparent and robust. Some experts frame this trend in a positive light. For example, sociologist Daniele Fanelli argues that the narrative of science “in crisis” is overblown, suggesting that scientific quality is not in free fall and that recent reproducibility efforts represent a healthy evolution of scientific practice rather than a disaster ([www.pnas.org](https://www.pnas.org/doi/full/10.1073/pnas.1708272114#:~:text=Efforts%20to%20improve%20the%20reproducibility,more%20accurate%2C%20inspiring%2C%20and%20compelling)). Likewise, psychologist Simine Vazire prefers to describe the changes as a **“credibility revolution”** instead of a crisis, emphasizing how improved methodologies and openness are strengthening the credibility of research ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=Collaboration%2C%20OSC%202015%29,to%20a%20%E2%80%9Ccredibility%20revolution%E2%80%9D%20highlighting)). In sum, there is broad agreement that reproducibility issues are real and must be addressed, but scholars differ in whether they view the situation as a dire crisis or as a constructive self-correction that will ultimately make science more reliable.

**Sources:**

1. **John P. A. Ioannidis (2005)** – *“Why Most Published Research Findings Are False.”* Ioannidis argues that a variety of biases and methodological weaknesses (such as small sample sizes, flexible analyses, and publication bias) make it likely that many published research claims are false ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC1182327/#:~:text=There%20is%20increasing%20concern%20that,is%20greater%20financial%20and%20other)). This landmark paper was an early warning about unreliable results in research, laying groundwork for the concerns that later fueled the replication crisis. **Link:** *PLoS Medicine* – [ncbi.nlm.nih.gov](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182327/)

2. **Monya Baker (2016)** – *“1,500 scientists lift the lid on reproducibility.”* Baker, a **Nature** journalist, surveyed 1,576 researchers about reproducibility. Over 70% reported failing to reproduce another scientist’s results, and about half had failed to reproduce their own experiments ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=More%20than%2070,questionnaire%20on%20reproducibility%20in%20research)). Notably, 52% of scientists characterized the situation as a **“significant crisis”**, although most still expressed trust in the published literature ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=The%20data%20reveal%20sometimes,that%20they%20still%20trust%20the)). Baker’s article also notes that prior analyses found only ~40% of psychology studies and ~10% of cancer biology studies could be replicated ([www.nature.com](https://www.nature.com/articles/533452a?code=1fe71eda-6af2-442d-84e3-afce01c30f46&error=cookies_not_supported#:~:text=Data%20on%20how%20much%20of,generally%20showing%20the%20most%20confidence)), highlighting the scope of the problem. **Link:** *Nature* News – [nature.com](https://www.nature.com/articles/533452a)

3. **Open Science Collaboration (2015)** – *“Estimating the reproducibility of psychological science.”* In this large-scale study led by Brian Nosek, 270 researchers attempted to replicate 100 findings from high-ranking psychology journals. The project found that less than half of the original findings could be successfully reproduced. Only 36% of the replication attempts yielded statistically significant results (versus 97% of the original studies), and replication effect sizes were much smaller on average ([www.science.org](https://www.science.org/doi/10.1126/science.aac4716#:~:text=0,that%20replication%20success%20was%20better)). This landmark paper provided concrete evidence of a reproducibility problem in psychology and spurred renewed calls for methodological reform. **Link:** *Science* – [science.org](https://www.science.org/doi/10.1126/science.aac4716)

4. **C. Glenn Begley & Lee M. Ellis (2012)** – *“Drug development: Raise standards for preclinical cancer research.”* Begley and Ellis reported that Amgen scientists could replicate just 6 of 53 “landmark” studies in preclinical cancer biology (∼11% success rate) ([brokenscience.org](https://brokenscience.org/raise-standards-for-preclinical-cancer-research/#:~:text=research%2C%20focusing%20on%20the%20lack,The)). They highlighted serious flaws in research practices – attributing the low reproducibility to poorly designed studies, publication pressures, and lack of methodological rigor – and called for higher standards in methodology and transparency. This finding showed that the replication crisis was not confined to psychology; it also affects biomedical research. **Link:** *Nature* commentary – [brokenscience.org](https://brokenscience.org/raise-standards-for-preclinical-cancer-research/)

5. **Daniele Fanelli (2018)** – *“Is science really facing a reproducibility crisis, and do we need it to?”* Fanelli contends that the narrative of a widespread “crisis” in science may be exaggerated ([www.pnas.org](https://www.pnas.org/doi/full/10.1073/pnas.1708272114#:~:text=Efforts%20to%20improve%20the%20reproducibility,more%20accurate%2C%20inspiring%2C%20and%20compelling)). He presents evidence that the quality and integrity of research have not decayed as drastically as some fear, and he argues for a more optimistic interpretation of recent changes. According to Fanelli, today’s emphasis on replication and transparency is an empowering improvement to scientific practice rather than a sign of imminent collapse. **Link:** *PNAS* (Opinion) – [pnas.org](https://www.pnas.org/doi/full/10.1073/pnas.1708272114)

6. **Simine Vazire (2018)** – *“Implications of the Credibility Revolution for Productivity, Creativity, and Progress.”* Vazire, a psychologist, calls the ongoing reforms a **“credibility revolution”** rather than a crisis. She highlights that the push for transparency, replication, and higher standards is strengthening the credibility of science. This view emphasizes the optimistic side of the replication crisis – focusing on how the scientific community is actively correcting itself and improving research practices ([plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/index.html#:~:text=Collaboration%2C%20OSC%202015%29,to%20a%20%E2%80%9Ccredibility%20revolution%E2%80%9D%20highlighting)). **Link:** *Perspectives on Psychological Science* (as cited in Stanford Encyclopedia) – [plato.stanford.edu](https://plato.stanford.edu/entries/scientific-reproducibility/)